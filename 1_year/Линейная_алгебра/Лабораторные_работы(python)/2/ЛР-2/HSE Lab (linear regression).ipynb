{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2\n",
    "\n",
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод наименьших квадратов: постановка задачи\n",
    "\n",
    "Рассмотрим систему уравнений $Xa = y$, в которой $a$ &mdash; столбец неизвестных. Её можно переписать в векторном виде\n",
    "$$x_1 a_1 + x_2 a_2 + \\ldots + x_k a_k = y,$$\n",
    "где $x_1,\\ldots,x_n$ &mdash; столбцы матрицы $X$. Таким образом, решить исходную систему означает найти линейную комбинацию векторов $x_1,\\ldots,x_n$, равную правой части. Но что делать, если такой линейной комбинации не существует? Геометрически это означает, что вектор $y$ не лежит в подпространстве $U = \\langle x_1,\\ldots, x_k\\rangle$. В этом случае мы можем найти *псевдорешение*: вектор коэффициентов $\\hat{a}$, для которого линейная комбинация $x_1 \\hat{a}_1 + x_2 \\hat{a}_2 + \\ldots + x_k \\hat{a}_k$ хоть и не равна в точности $y$, но является наилучшим приближением &mdash; то есть ближайшей к $y$ точкой $\\hat{y}$ подпространства $U$ (иными словами, ортогональной проекцией $y$ на это подпростанство). Итак, цель наших исканий можно сформулировать двумя эквивалентными способами:\n",
    "\n",
    "1. Найти вектор $\\hat{a}$, для которого длина разности $|X\\hat{a} - y|$ минимальна (отсюда название \"метод наименьших квадратов\");\n",
    "2. Найти ортогональную проекцию $\\hat{y}$ вектора $y$ на подпространство $U$ и представить её в виде $X\\hat{a}$.\n",
    "\n",
    "Далее мы будем предполагать, что векторы $x_1,\\ldots,x_n$ линейно независимы (если нет, то сначала имеет смысл выделить максимальную линейно независимую подсистему).\n",
    "\n",
    "На лекциях было показано, что проекция вектора $y$ на подпространство $U = \\langle x_1,\\ldots, x_k\\rangle$ записывается в виде\n",
    "$$\\hat{y} = X\\left(X^TX\\right)^{-1}X^Ty,$$\n",
    "и, соответственно, искомый вектор $\\hat{a}$ равен\n",
    "$$\\hat{a} = \\left(X^TX\\right)^{-1}X^Ty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача линейной регрессии\n",
    "\n",
    "Начнём с примера. Допустим, вы хотите найти зависимость среднего балла S студента ФКН от его роста H, веса W, длины волос L и N &mdash; количества часов, которые он ежедневно посвящает учёбе. Представьте, что мы измерили все эти параметры для $n$ студентов и получили наборы значений $S_1,\\ldots, S_n$, $H_1,\\ldots, H_n$ и так далее.\n",
    "\n",
    "Тут можно подбирать много разных умных моделей, но начать имеет смысл с самой простой, линейной:\n",
    "$$S = a_1H + a_2W + a_3L + a_4N + a_5.$$\n",
    "Конечно, строгой линейной зависимости нет (иначе можно было бы радостно упразднить экзамены), но мы можем попробовать подобрать коэффициенты $a_1, a_2, a_3, a_4, a_5$, для которых отклонение правой части от наблюдаемых было бы наименьшим:\n",
    "$$\\sum_{i=1}^n\\left(S_i - ( a_1H_i + a_2W_i + a_3L_i + a_4N_i + a_5)\\right)^2 \\longrightarrow \\min$$\n",
    "И сразу видно, что мы получили задачу на метод наименьших квадратов! А именно, у нас\n",
    "$$X =\n",
    "\\begin{pmatrix}\n",
    "H_1 & W_1 & L_1 & N_1 & 1\\\\\n",
    "H_2 & W_2 & L_2 & N_2 & 1\\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "H_n & W_n & L_n & N_n & 1\n",
    "\\end{pmatrix},\\qquad y=\n",
    "\\begin{pmatrix}\n",
    "S_1\\\\ S_2\\\\ \\vdots \\\\ S_n\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Решая эту задачу с помощью уже известных формул, получаем оценки коэффициентов $\\hat{a}_i$ ($i = 1\\ldots,5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проговорим общую постановку задачи линейной регрессии. У нас есть $k$ переменных $x_1,\\ldots,x_k$ (\"регрессоров\"), через которые мы хотим выразить \"объясняемую переменную\" $y$:\n",
    "$$y = a_1x_1 + a_2x_2 + \\ldots + a_kx_k$$\n",
    "Значения всех переменных мы измерили $n$ раз (у $n$ различных объектов, в $n$ различных моментов времени &mdash; это зависит от задачи). Подставим эти данные в предыдущее равенство:\n",
    "$$\\begin{pmatrix}\n",
    "y_1\\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix} = \n",
    "a_1\\begin{pmatrix}\n",
    "x_{11} \\\\ x_{21} \\\\ \\vdots \\\\ x_{n1} \\end{pmatrix} + a_2\\begin{pmatrix}\n",
    "x_{12} \\\\ x_{22} \\\\ \\vdots \\\\ x_{n2} \\end{pmatrix} + \\ldots + a_k\\begin{pmatrix}\n",
    "x_{1k} \\\\ x_{2k} \\\\ \\vdots \\\\ x_{nk} \\end{pmatrix}$$\n",
    "(здесь $x_{ij}$ &mdash; это значение $j$-го признака на $i$-м измерении). Это удобно переписать в матричном виде:\n",
    "$$\\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\ldots & x_{1k}\\\\\n",
    "x_{21} & x_{22} & \\ldots & x_{2k}\\\\\n",
    "\\dots & \\dots & \\dots & \\dots\\\\\n",
    "x_{n1} & x_{n2} & \\ldots & x_{nk}\n",
    "\\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix}\n",
    "a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_k\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}$$\n",
    "или коротко $Xa = y$. Поскольку на практике эта система уравнений зачастую не имеет решения (ибо зависимости в жизни редко бывают действительно линейными), методом наименьших квадратов ищется псевдорешение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества. Обобщающая способность. Обучение и тест \n",
    "\n",
    "После того, как вы построили регрессию и получили какую-то зависимость объясняемой переменной от регрессоров, настаёт время оценить качество регрессии. Есть много разных функционалов качества; мы пока будем говорить только о самом простом и очевидном из них &mdash; о среднеквадратичной ошибке (mean square error). Она равна\n",
    "$$\\frac1{n}|X\\hat{a} - y|^2 = \\frac1{n}\\sum_{i=1}^n\\left(\\hat{a}_1x_{i1} + \\hat{a}_2x_{i2} + \\ldots + \\hat{a}_kx_{ik} - y_i\\right)^2$$\n",
    "\n",
    "В целом хочется искать модели с наименьшей mean square error на имеющихся данных. Однако слишком фанатичная гонка за минимизацией ошибки может привести к печальным последствиям. Например, если мы приближаем функцию одной переменной по значениям в $n$ точках, то наилучшей с точки зрения этой ошибки моделью будет многочлен $(n-1)$-й степени, для которого эта ошибка будет равна нулю.  Тем не менее, вряд ли истинная зависимость имеет вид многочлена большой степени. Более того, значения вам скорее всего даны с погрешностью, то есть вы подогнали вашу модель под свои зашумлённые данные, но на любых других данных (то есть в других точках) точность, скорее всего, окажется совсем не такой хорошей. Этот эффект называют **переобучением**; говорят также, что **обобщающая способность** модели оказалась скверной.\n",
    "\n",
    "Чтобы не попадать в эту ловушку, данные обычно делят на обучающие (по которым строят модель и оценивают коэффициенты) и тестовые. Лучшей стоит счесть ту модель, для которой значение функционала качества будет меньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Метод наименьших квадратов (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте файлы ``train.txt`` и ``test.txt``. В первом из них находится обучающая выборка, а во втором &mdash; тестовая. Каждый из файлов содержит два столбца чисел, разделённых пробелами: в первом &mdash; $n$ точек (значения аргумента $x$), во втором &mdash; значения некоторой функции $y = f(x)$ в этих точках, искажённые случайным шумом. Ваша задача &mdash; по обучающей выборке подобрать функцию $y = g(x)$, пристойно приближающую неизвестную вам зависимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим обучающие и тестовые данные (не забудьте ввести правильный путь!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = numpy.loadtxt('.../train.txt', delimiter=',')\n",
    "data_test = numpy.loadtxt('.../test.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим значения $x$ и $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data_train[:,0]\n",
    "y_train = data_train[:,1]\n",
    "\n",
    "# Сделайте то же для тестовой выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите с помощью метода наименьших квадратов линейную функцию ($y = kx + b$), наилучшим образом приближающую неизвестную зависимость. Полезные функции: ``numpy.ones(n)`` для создания массива из единиц длины $n$ и ``numpy.concatenate((А, В), axis=1)`` для слияния двух матриц по столбцам (пара ``А`` и ``В`` превращается в матрицу ``[A B]``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте на плоскости точки $(x_i, y_i)$ и полученную линейную функцию. Глядя на данные, подумайте, многочленом какой степени можно было бы лучше всего приблизить эту функцию. Найдите этот многочлен и нарисуйте его график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для $k = 1,2,3,\\ldots,10$ найдите многочлен $\\hat{f}_k$ степени $k$, наилучшим образом приближающий неизвестную зависимость. Для каждого из них найдите среднеквадратическую ошибку на обучающих данных и на тестовых данных: $\\frac1{n}\\sum_{i=1}^n\\left( \\hat{f}_k(x_i) - y_i \\right)^2$ (в первом случае сумма ведётся по парам $(x_i, y_i)$ из обучающих данных, а во втором - по парам из тестовых данных).\n",
    "\n",
    "Для $k = 1,2,3,4,6$ напечатайте коэффициенты полученных многочленов и нарисуйте их графики на одном чертеже вместе с точками $(x_i, y_i)$ (возможно, график стоит сделать побольше; это делается командой `plt.figure(figsize=(width, height))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что происходит с ошибкой при росте степени многочлена? Казалось бы, чем больше степень, тем более сложным будет многочлен и тем лучше он будет приближать нашу функцию. Подтверждают ли это ваши наблюдения? Как вам кажется, чем объясняется поведение ошибки на тестовых данных при $k = 10$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Линейная регрессия (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте файлы ``flats_moscow_mod.txt`` и ``flats_moscow_description.txt``. В первом из них содержатся данные о квартирах в Москве. Каждая строка содержит шесть характеристик некоторой квартиры, разделённые знаками табуляции; в первой строке записаны кодовые названия характеристик. Во втором файле приведены краткие описания признаков. Вашей задачей будет построить с помощью метода наименьших квадратов (линейную) зависимость между ценой квартиры и остальными доступными параметрами.\n",
    "\n",
    "С помощью известных вам формул найдите регрессионные коэффициенты. Какой смысл имеют их знаки? Согласуются ли они с вашими представлениями о жизни?\n",
    "\n",
    "Оцените качество приближения, вычислив среднеквадратическую ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Усложнение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, никто не гарантирует, что объясняемая переменная (цена квартиры) зависит от остальных характеристик именно линейно. Зависимость может быть, например, квадратичной или логарифмической; больше того, могут быть важны не только отдельные признаки, но и их комбинации. Это можно учитывать, добавляя в качестве дополнительных признаков разные функции от уже имеющихся характеристик: их квадраты, логарифмы, попарные произведения.\n",
    "\n",
    "В этом задании вам нужно постараться улучшить качество модели, добавляя дополнительные признаки, являющиеся функциями от уже имеющихся. Но будьте осторожны: чрезмерное усложнение модели будет приводить к переобучению. \n",
    "\n",
    "**Сравнение моделей**\n",
    "\n",
    "Когда вы построите новую модель, вам захочется понять, лучше она или хуже, чем изначальная. Проверять это на той же выборке, на которой вы обучались, бессмысленно и даже вредно (вспомните пример с многочленами: как прекрасно падала ошибка на обучающей выборке с ростом степени!). Поэтому вам нужно будет разделить выборку на обучающую и тестовую. Делать это лучше случайным образом (ведь вы не знаете, как создатели датасета упорядочили объекты); рекомендуем вам для этого функцию `sklearn.model_selection.train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Регуляризация (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что задача линейной регрессии формулируется как задача нахождения проекции вектора значений объясняемой переменной $y$ на линейную оболочку $\\langle x_1,\\ldots,x_k\\rangle$ векторов значений регрессоров. Если векторы $x_1,\\ldots,x_k$ линейно зависимы, то матрица $X^TX$ вырожденна и задача не будет решаться (то есть будет, но не с помощью приведённой выше формулы). В жизни, по счастью, различные признаки редко бывают *в точности* линейно зависимы, однако во многих ситуациях они скоррелированы и становятся \"почти\" линейно зависимыми. Таковы, к примеру, зарплата человека, его уровень образования, цена машины и суммарная площадь недвижимости, которой он владеет. В этом случае матрица $X^TX$ будет близка к вырожденной, и это приводит к численной неустойчивости и плохому качеству решений; как следствие, будет иметь место переобучение. Один из симптомов этой проблемы &mdash; необычно большие по модулю компоненты вектора $a$.\n",
    "\n",
    "Есть много способов борьбы с этим злом. Один из них &mdash; регуляризация. Сейчас мы рассмотрим одну из её разновидностей &mdash; **L2-регуляризацию**. Идея в том, чтобы подправить матрицу $X^TX$, сделав её \"получше\". Например, это можно сделать, заменив её на $(X^TX + \\lambda E)$, где $\\lambda$ &mdash; некоторый скаляр. Пожертвовав точностью на обучающей выборке, мы тем не менее получаем численно более стабильное псевдорешение $a = (X^TX + \\lambda E)^{-1}X^Ty$ и снижаем эффект переобучения. Параметр $\\lambda$ нужно подбирать, и каких-то универсальных способов это делать нет, но зачастую можно его подобрать таким, чтобы ошибка на тестовой выборке падала. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте вспомним первую задачу. Если вы её сделали, то помните, что ошибка аппроксимации многочленом шестой степени довольно высокая. Убедитесь, что, используя регуляризацию с хорошо подобранным коэффициентом $\\lambda$, ошибку на тестовой выборке можно сделать не больше, чем для многочлена оптимальной степени в модели без регрессии. Для этого $\\lambda$ сравните $\\det(X^TX)$ и $\\det(X^TX + \\lambda E)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте на одном чертеже графики многочленов шестой степени, приближающих неизвестную функцию, для модели с регуляризацией и без. Чем первый из них выгодно отличается от второго?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте доказать, что вектор $a = (X^TX + \\lambda E)^{-1}X^Ty$ является решением задачи\n",
    "\n",
    "$$|Xa - y|^2 + \\lambda|a|^2\\rightarrow\\min$$\n",
    "\n",
    "Интуитивно это можно понимать так: мы ищем компромисс между минимизацией длины разности $|Xa - y|$ (то есть точностью решения задачи регрессии) и тем, чтобы компоненты вектора $a$ не становились слишком большими по модулю.\n",
    "\n",
    "---\n",
    "\n",
    "**Ваше решение напишите прямо здесь**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Онлайн-обучение линейной регрессии (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Раньше мы работали в ситуации, когда объекты $x_i$ и значения $y_i$ даны с самого начала и всегда доступны. Допустим теперь, что пары $(x_i, y_i)$ поступают к нам по одной и мы не можем себе позволить хранить их все в памяти (это может быть актуально, например, если вы пытаетесь обучить модель на устройстве со сравнительно небольшим количеством оперативной памяти: скажем, на мобильном телефоне или на бортовом компьютере спутника связи). В этом случае нам нужно уметь решать следующую задачу:\n",
    "\n",
    "**Известно:** решение задачи регрессии для датасета $(x_1, y_1),\\ldots,(x_t,y_t)$;\n",
    "\n",
    "**На вход поступает:** новая пара $(x_{t+1}, y_{t+1})$;\n",
    "\n",
    "**Требуется:** быстро (за время, не зависящее от $t$) отыскать решение задачи регрессии для расширенного датасета $(x_1, y_1),\\ldots,(x_t,y_t),(x_{t+1}, y_{t+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту задачу мы будем решать в два этапа.\n",
    "\n",
    "**Этап 1.** Введём обозначения $X_{(t)} = (x_1\\ldots x_t)$ и $y_{(t)} = (y_1,\\ldots,y_t)^T$. Тогда, как мы хорошо помним, решение задачи регрессии для датасета $(x_1, y_1),\\ldots,(x_t,y_t)$ имеет вид $\\hat{a}_{(t)} = \\left(X^T_{(t)}X_{(t)}\\right)^{-1}X^T_{(t)}y_{(t)}$. Размеры матриц $X^T_{(t)}X_{(t)}$ и $X^T_{(t)}y_{(t)}$ не зависят от $t$, поэтому их мы, пожалуй, можем себе позволить хранить в памяти.\n",
    "\n",
    "И вот ваше первое задание в этом разделе: придумайте алгоритм, принимающий на вход матрицы $X^T_{(t)}X_{(t)}$ и $X^T_{(t)}y_{(t)}$, а также пару $(x_{t+1}, y_{t+1})$ и вычисляющий матрицы $X^T_{(t+1)}X_{(t+1)}$ и $X^T_{(t+1)}y_{(t+1)}$. Сложность вашего алгоритма не должна зависеть от $t$!\n",
    "\n",
    "--\n",
    "\n",
    "**Описание вашего алгоритма напишите прямо здесь**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Этап 2.** Теперь настало время написать немного кода и порисовать красивые картинки. Вам нужно будет реализовать симуляцию онлайн-обучения регрессии для задачи приближения функции (в данном случае $f_{true}(x) = 2x\\sin(x) + x^2 - 1$; все значения искажены небольшим нормальным шумом) многочленом степени не выше 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "f_true = lambda x: 2*x*np.sin(5*x) + x**2 - 1 # this is the true function\n",
    "\n",
    "# We need this to make the plot of f_true:\n",
    "x_grid = np.linspace(-2,5,100) # 100 linearly spaced numbers\n",
    "x_grid_enl = np.hstack((x_grid.reshape((100,1))**j for j in range(6)))\n",
    "y_grid = f_true(x_grid)\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    x_new = np.random.uniform(-2, 5)\n",
    "    y_new = f_true(x_new) + 2*np.random.randn()\n",
    "    \n",
    "    # your code goes here\n",
    "    \n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i+1)%5==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(x_grid,y_grid, color='blue', label='true f')\n",
    "        plt.scatter(x_new, y_new, color='red')\n",
    "        \n",
    "        # your code goes here\n",
    "        y_pred = #...\n",
    "        \n",
    "        plt.scatter(x_grid, y_pred, color='orange', linewidth=5, label='predicted f')\n",
    "        \n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление. QR-разложение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QR-разложением** матрицы $A$ (не обязательно квадратной) мы будем называть её представление в виде $A = QR$, где $Q$ &mdash; матрица с ортонормированными столбцами, а $R$ &mdash; верхнетреугольная матрица.\n",
    "\n",
    "Смысл QR-разложения следующий. Пусть $a_1,\\ldots,a_m$ &mdash; столбцы матрицы $A$, $q_1,\\ldots,q_t$ &mdash; столбцы матрицы $Q$. Тогда $q_1,\\ldots,q_t$ &mdash; это ортонормированный базис в подпространстве, являющемся линейной оболочкой векторов $a_1,\\ldots,a_m$, а в матрице $R$ записаны коэффициенты, с помощью которых $a_i$ выражаются через $q_1,\\ldots,q_t$.\n",
    "\n",
    "Находить QR-разложение заданной матрицы можно разными способами. Мы познакомим вас не с самым лучшим из них, но по крайней мере с наиболее простым концептуально. Заметим, что ортогональный базис линейной оболочки можно найти с помощью ортогонализации Грама-Шмидта. При этом коэффициенты из матрицы $R$ получаются в качестве побочного продукта этого процесса:\n",
    "\n",
    "```python\n",
    "for j = 1...n:\n",
    "    q_j = a_j\n",
    "    for i = 1,...,j-1:\n",
    "        r_ij = (q_i, a_j)\n",
    "        q_j = q_j - r_ij * q_i\n",
    "    r_jj = |q_j|\n",
    "    if r_jj == 0: # a_j in <a_1,...,a_j-1>\n",
    "        # What would you do in this case?..\n",
    "    q_j = q_j / r_jj\n",
    "```\n",
    "\n",
    "Для нахождения QR-разложения вы можете использовать библиотечную функцию `scipy.linalg.qr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку лабораторная про линейную регрессию, не так-то просто замять вопрос о том, какое же отношение QR-разложение имеет к задаче регрессии. Упомянем одно из возможных применений.\n",
    "\n",
    "Допустим, мы нашли QR-разложение матрицы $X$, а именно: $X = QR$. Тогда\n",
    "$$X^TX = (QR)^T(QR) = R^TQ^TQR = R^TR$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в задаче регрессии матрица $X$ обычного полного ранга (то есть её столбцы линейно независимы), матрица $R$ будет квадратной. Благодаря этому нашу обычную формулу для набора регрессионных коэффициентов $\\hat{a}$ можно переписать в следующем виде:\n",
    "\n",
    "$$\\hat{a} = (X^TX)^{-1}X^Ty = (R^TR)^{-1}(QR)^Ty = R^{-1}(R^T)^{-1}R^TQ^Ty = R^{-1}Q^Ty$$\n",
    "\n",
    "Как видите, формула стала проще. Более того, зачастую обращение матрицы $R$ может быть численно более устойчиво, чем обращение матрицы $X^TX$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
